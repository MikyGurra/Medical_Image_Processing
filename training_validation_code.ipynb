{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"s-0J3ycmCDxL"},"outputs":[],"source":["# SECTION 1: Initial Setup and libraries import\n","\n","from google.colab import drive\n","drive.mount('/content/drive')               # Google Drive\n","import os\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","# Deep Learning libraries\n","import torch, torchvision\n","print(torch.__version__, torch.cuda.is_available())\n","\n","# Working directory\n","current_dir = '/content/drive/MyDrive/Progetto/'\n","\n","# SECTION 2: U-Net definition\n","\n","import torch.nn as nn                       # Modules for defining neural networks\n","import torch.nn.functional as F             # Operative functions\n","\n","class DoubleConv(nn.Module):\n","    \"\"\"Due convoluzioni consecutive con BatchNorm e ReLU\"\"\"\n","    def __init__(self, in_channels, out_channels):\n","        super(DoubleConv, self).__init__()\n","        self.double_conv = nn.Sequential(   # Sequence of layers\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), # 2D Convolution\n","            nn.BatchNorm2d(out_channels),   # Batch Normalization\n","            nn.ReLU(inplace=True),          # ReLu\n","\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","\n","class UNet(nn.Module):\n","    \"\"\"Definizione della rete UNet con encoder-decoder e skip connections\"\"\"\n","    def __init__(self, in_channels=1, out_channels=1, init_filters=24, depth=4, bilinear=True):\n","        super(UNet, self).__init__()\n","        self.depth = depth\n","        self.down_layers = nn.ModuleList()  # List of the encoder layers\n","        self.up_layers = nn.ModuleList()    # List of the decoder layers\n","        self.pool = nn.MaxPool2d(2)         # Pooling to reduce dimensions\n","\n","        # Encoder\n","        filters = init_filters\n","        for d in range(depth):\n","            conv = DoubleConv(in_channels, filters)\n","            self.down_layers.append(conv)\n","            in_channels = filters\n","            filters *= 2                            # Doubles the filters\n","\n","        # Bottleneck\n","        self.bottleneck = DoubleConv(in_channels, filters)\n","\n","        # Decoder\n","        for d in range(depth):\n","            filters //= 2\n","            if bilinear:\n","                up = nn.Sequential(\n","                    nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n","                    nn.Conv2d(filters * 2, filters, kernel_size=1)\n","                )\n","            else:\n","                up = nn.ConvTranspose2d(filters * 2, filters, kernel_size=2, stride=2)\n","            self.up_layers.append(nn.ModuleDict({\n","                'up': up,\n","                'conv': DoubleConv(filters * 2, filters)\n","            }))\n","\n","        # Output layer\n","        self.out_conv = nn.Conv2d(init_filters, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        skip_connections = []               # List for skip connections\n","        for down in self.down_layers:       # Encoder\n","            x = down(x)\n","            skip_connections.append(x)\n","            x = self.pool(x)\n","\n","        x = self.bottleneck(x)              # Bottleneck\n","\n","        for i in range(self.depth):         # Decoder\n","            skip = skip_connections[-(i+1)] # Restores skip connection\n","            up = self.up_layers[i]['up'](x)\n","            if up.size() != skip.size():    # Managing size mismatches\n","                up = F.interpolate(up, size=skip.shape[2:])\n","            x = torch.cat([skip, up], dim=1)\n","            x = self.up_layers[i]['conv'](x)\n","\n","        return self.out_conv(x)             # Output finale\n","\n","\n","# SECTION 3: Model testing\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Selection of GPU if is avaiable\n","model = UNet(in_channels=1, out_channels=1, init_filters=24, depth=4)\n","model.to(device)                                                      # Move to device\n","summary(model, input_size=(1, 1024, 1024))                            # Show the model\n","\n","class DiceLoss(nn.Module): # loss function based on the dice coefficient\n","\n","\n","    def __init__(self, smooth=1e-6):\n","        super(DiceLoss, self).__init__()\n","        self.smooth = smooth\n","\n","    def forward(self, inputs, targets):\n","        inputs = torch.sigmoid(inputs)      # Convert logits in probability\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","        intersection = (inputs * targets).sum()\n","        dice = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)\n","        return 1 - dice                     # Loss = 1 - Dice\n","\n","\n","# SECTION 4: Training Configuration\n","\n","input_size = (1024,1024)                      # Input image dimensions\n","in_channels = 1                               # Number of input channels\n","out_channels = 1                              # Number of output channels (1=binary segmentation)\n","init_filters = 16\n","depth = 4\n","criterion = DiceLoss()                        # Loss Function\n","n_epochs = 30\n","batch_size = 7\n","learning_rate = 0.0003\n","checkpoint_freq = 1                           # Checkpoints' save frequency\n","checkpoint_dir = os.path.join(current_dir,'CODICE ONLINE POST TUNING','checkpoints') ##################################################### CHANGE DIRECTORY\n","\n","if not os.path.exists(checkpoint_dir):        # Create directory if it doesn't exist\n","    os.makedirs(checkpoint_dir)\n","\n","\n","# SECTION 5: Dataset e DataLoader\n","\n","train_img_dir = os.path.join(current_dir,'Dataset_vessel_stu','train','image')\n","train_mask_dir = os.path.join(current_dir,'Dataset_vessel_stu','train','manual_py')\n","val_img_dir = os.path.join(current_dir,'Dataset_vessel_stu','val','image')\n","val_mask_dir = os.path.join(current_dir,'Dataset_vessel_stu','val','manual_py')\n","\n","import json\n","\n","params = {\n","    \"input_size\": input_size,\n","    \"in_channels\": in_channels,\n","    \"out_channels\": out_channels,\n","    \"init_filters\": init_filters,\n","    \"depth\": depth,\n","    \"n_epochs\": n_epochs,\n","    \"batch_size\": batch_size,\n","    \"learning_rate\": learning_rate,\n","    \"checkpoint_freq\": checkpoint_freq,\n","}\n","\n","params_path = os.path.join(checkpoint_dir, 'training_params.json')\n","with open(params_path, 'w') as f:\n","    json.dump(params, f, indent=4)\n","print(f\"Training parameters saved to {params_path}\")\n","\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import cv2\n","from skimage import exposure\n","\n","class RetinalDataset(Dataset):\n","\n","    def __init__(self, image_dir, mask_dir, transform=None,multiplier=1):\n","      self.image_dir = image_dir\n","      self.mask_dir = mask_dir\n","      self.transform = transform\n","      self.image_list = sorted(os.listdir(image_dir))\n","      self.mask_list = sorted(os.listdir(mask_dir))\n","      self.image_list = self.image_list * multiplier\n","      self.mask_list = self.mask_list * multiplier\n","      # Check that the lists have the same lenght\n","      assert len(self.image_list) == len(self.mask_list)\n","\n","    def __len__(self):\n","      return len(self.image_list)\n","    def __getitem__(self, idx):\n","      img_path = os.path.join(self.image_dir, self.image_list[idx])\n","      mask_path = os.path.join(self.mask_dir, self.mask_list[idx])\n","\n","      #Convert the image from RGB to 'grayscale whit no blue channel'\n","      image = np.array(Image.open(img_path).convert('RGB'))\n","      R = image[:,:,0].astype(np.float32)\n","      G = image[:,:,1].astype(np.float32)\n","      img_RG = (0.337 * R + 0.663 * G).astype(np.uint8)\n","      #Apply the preprocessing\n","      #Apply Gaussian filter (kernel_size = 3; sigma =1)\n","      img = cv2.GaussianBlur(img_RG, (3,3), 1)\n","      #Apply gamma correction (gamma = 0.9)\n","      img = img/255\n","      img_gamma = exposure.adjust_gamma(img, gamma=0.9)\n","      img_gamma = (img_gamma * 255).astype(np.uint8)\n","\n","      mask = np.array(Image.open(mask_path).convert('L'))\n","\n","      if self.transform:\n","        # Albumentations processes the image and mask together\n","        augmented = self.transform(image=img_gamma, mask=mask)\n","        image = augmented['image']\n","        mask = augmented['mask']\n","\n","      # Conversion of the mask to a binary float (0.0 or 1.0)\n","      # Note: The image has already been converted to a float tensor by ToTensorV2 or ToFloat in the pipeline\n","      mask = (mask > 0).float()\n","\n","      if mask.ndim == 2:\n","        mask = mask.unsqueeze(0)\n","\n","      return image, mask\n","\n","\n","# SECTION 6: Trasformations and DataLoader. This section defines the trasformations applied to the images\n","#             and initializes dataset and dataloader.\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","# Training: Resize + Augmentation + Scaling [0,1]\n","train_transform = A.Compose([\n","    A.Resize(height=input_size[0], width=input_size[1]), # Image resized here\n","    A.HorizontalFlip(p=0.6),\n","    A.VerticalFlip(p=0.6),\n","    A.RandomRotate90(p=0.6),\n","    A.Transpose(p=0.6),\n","    # Important: It scales pixel values to the [0, 1] range by dividing by 255, but without subtracting mean/std\n","    A.ToFloat(max_value=255.0),\n","    ToTensorV2()\n","])\n","\n","# Validation: Resize + Scaling [0,1]\n","val_transform = A.Compose([\n","    A.Resize(height=input_size[0], width=input_size[1]),\n","    A.ToFloat(max_value=255.0),\n","    ToTensorV2()\n","])\n","\n","# Dataset initialization\n","train_dataset = RetinalDataset(train_img_dir, train_mask_dir, transform=train_transform, multiplier=3) #the training set is multiplied by 3\n","val_dataset = RetinalDataset(val_img_dir, val_mask_dir, transform=val_transform,multiplier=1)\n","\n","# Dataloader initialization\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","\n","# SECTION 7: Model initalization and optimizer\n","\n","import torch.optim as optim\n","\n","model = UNet(in_channels, out_channels, init_filters, depth).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)            # Adam optimizer\n","\n","\n","# SECTION 8: clDice metric definition\n","\n","from skimage.morphology import skeletonize\n","\n","def calculate_cldice(y_true, y_pred):\n","    \"\"\"\n","    Calcola il clDice basato sulla formula:\n","    clDice = 2 * (TP_prec * TS_sens) / (TP_prec + TS_sens)\n","    \"\"\"\n","    t_mask = y_true.astype(bool)          # Convert ground truth mask into boolean type\n","    p_mask = y_pred.astype(bool)          # Convert predicted mask into boolean type\n","\n","    t_skeleton = skeletonize(t_mask)\n","    p_skeleton = skeletonize(p_mask)\n","\n","    overlap_pred_skel_true_mask = p_skeleton & t_mask   #Intersection between predicted skelethon and groud truth mask\n","    tp_len = np.sum(p_skeleton)\n","    if tp_len == 0:\n","        tprec = 0\n","    else:\n","        tprec = np.sum(overlap_pred_skel_true_mask) / tp_len\n","\n","    overlap_true_skel_pred_mask = t_skeleton & p_mask   #Intersection between ground truth skelethon and predicted mask\n","    ts_len = np.sum(t_skeleton)\n","    if ts_len == 0:\n","        tsens = 0\n","    else:\n","        tsens = np.sum(overlap_true_skel_pred_mask) / ts_len\n","\n","    if tprec + tsens == 0:                              # divison by zero is avoided\n","        return 0.0\n","\n","    cldice = 2.0 * tprec * tsens / (tprec + tsens)\n","    return cldice\n","\n","from skimage.measure import label\n","from skimage.morphology import skeletonize\n","\n","def clean_by_skeleton_length(mask, min_length): # post processing function to remove small component\n","    labeled, num = label(mask.astype(np.uint8), return_num=True)\n","\n","    refined = np.zeros_like(mask, dtype=np.uint8)\n","\n","    for lab in range(1, num + 1):\n","        comp_mask = (labeled == lab)\n","\n","        # component skeleton\n","        skel = skeletonize(comp_mask)\n","        length = np.sum(skel)\n","\n","        # elimination of components that are too short\n","        if length >= min_length:\n","            refined[comp_mask] = 1\n","\n","    return refined\n","\n","# SECTION 9: Training and validation\n","\n","from sklearn.metrics import jaccard_score, f1_score, accuracy_score, precision_score, recall_score\n","\n","#List to save the trend of the metrics\n","train_losses = []\n","val_losses = []\n","val_dscs = []\n","val_cldices = []\n","val_precs = []\n","val_recalls = []\n","\n","\n","#  batch size of 7 and set accumulation_steps to 2, you simulate a batch size of 14\n","accumulation_steps = 2\n","\n","for epoch in range(n_epochs):\n","\n","    model.train()\n","    train_loss = 0\n","\n","    #Reset the gradients BEFORE starting the batch loop\n","    optimizer.zero_grad()\n","\n","    for i, (images, masks) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs} - Training\")):\n","        images = images.to(device)\n","        masks = masks.to(device)\n","\n","        # Note: Not calling optimizer.zero_grad() here because we want the gradients to accumulate\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, masks)        # compute the full loss\n","\n","        # Normalization --> the loss is divided by the number of accumulation steps\n","        # This ensures that the accumulated gradients form an average\n","        loss = loss / accumulation_steps\n","\n","        loss.backward()                         # Backpropagation\n","\n","        # Conditional Step --> the weights are updated every accumulation_steps' iterations\n","        if (i + 1) % accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Handling of the last batch\n","        elif (i + 1) == len(train_loader):\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Correct logging --> Since we divided the loss for gradient accumulation, we multiply it back to display the actual value\n","        train_loss += (loss.item() * accumulation_steps) * images.size(0)\n","\n","    train_loss /= len(train_loader.dataset)\n","    print(f\"Epoch {epoch+1} Train Loss: {train_loss:.4f}\")\n","\n","   # Validation --> eval modality\n","    model.eval()\n","    val_loss = 0.0\n","\n","    # List to accumulate the metrics image by image\n","    epoch_dscs = []\n","    epoch_cldices = []\n","    epoch_precs = []\n","    epoch_recalls = []\n","\n","    with torch.no_grad():                       # Disable gradient\n","        for images, masks in val_loader:\n","            images = images.to(device)\n","            masks = masks.to(device)\n","            outputs = model(images)\n","\n","            loss = criterion(outputs, masks)\n","            val_loss += loss.item() * images.size(0)\n","\n","            if out_channels == 1:               # Binary segmentation\n","                probs = torch.sigmoid(outputs)\n","\n","                preds = (probs > 0.5).long()    # Threshold 0.5\n","                true = masks.long()\n","            else:                               # Multi-class segmentation --> we never use it\n","                preds = torch.argmax(outputs, dim=1)\n","                true = masks.long()\n","\n","            preds_np = preds.squeeze().cpu().numpy()    # It converts prediction in NumPy\n","\n","\n","            #if batch has a sigle image, it adjusts the shape\n","            if preds_np.ndim == 2:\n","                preds_np = np.expand_dims(preds_np, axis=0)\n","\n","            # Post-processing: removal of elements that are too small by checking the length of the skeleton\n","            refined_preds = []\n","            for i in range(preds_np.shape[0]):\n","                refined = clean_by_skeleton_length(\n","                    preds_np[i], min_length=40\n","                )\n","                refined_preds.append(refined)\n","\n","            preds_np = np.stack(refined_preds, axis=0)\n","\n","            # Ground truth\n","            true_np = true.squeeze().cpu().numpy()\n","            if true_np.ndim == 2:\n","                true_np = np.expand_dims(true_np, axis=0)\n","\n","            # CALCULATE METRICS FOR EACH IMAGE IN THE BATCH\n","            for i in range(preds_np.shape[0]):\n","                # Flatten masks to evaluate pixel‑wise metrics (required by sklearn)\n","                flat_true = true_np[i].flatten()\n","                flat_pred = preds_np[i].flatten()\n","\n","                # Compute standard segmentation metrics for the current sample\n","                dsc_img = f1_score(flat_true, flat_pred, average='binary')          # Dice score\n","                prec_img = precision_score(flat_true, flat_pred, average='binary')  # Precision\n","                rec_img = recall_score(flat_true, flat_pred, average='binary')      # Recall\n","\n","                # Compute clDice, which evaluates topological consistency of the vessel tree\n","                cldice_img = calculate_cldice(true_np[i], preds_np[i])\n","\n","                # Store per‑image metrics for later averaging across the epoch\n","                epoch_dscs.append(dsc_img)\n","                epoch_precs.append(prec_img)\n","                epoch_recalls.append(rec_img)\n","                epoch_cldices.append(cldice_img)\n","\n","\n","    val_loss /= len(val_loader.dataset)         # Validation Loss Mean\n","\n","    # Compute mean and standard deviation for each metric\n","\n","    dsc = np.mean(epoch_dscs)\n","    dsc_std = np.std(epoch_dscs)\n","\n","    cldice = np.mean(epoch_cldices)\n","    cldice_std = np.std(epoch_cldices)\n","\n","    precision = np.mean(epoch_precs)\n","    prec_std = np.std(epoch_precs)\n","\n","    recall = np.mean(epoch_recalls)\n","    rec_std = np.std(epoch_recalls)\n","\n","    # Print as Mean ± Std\n","    print(f\"Epoch {epoch+1} Val Loss: {val_loss:.4f}\")\n","    print(f\"Metrics (Mean ± Std):\")\n","    print(f\"DSC:    {dsc:.4f} ± {dsc_std:.4f}\")\n","    print(f\"clDice: {cldice:.4f} ± {cldice_std:.4f}\")\n","    print(f\"Prec:   {precision:.4f} ± {prec_std:.4f}\")\n","    print(f\"Recall: {recall:.4f} ± {rec_std:.4f}\")\n","\n","    # Append to the origianle list (we save only the mean value)\n","    train_losses.append(train_loss)\n","    val_losses.append(val_loss)\n","    val_dscs.append(dsc)\n","    val_cldices.append(cldice)\n","    val_precs.append(precision)\n","    val_recalls.append(recall)\n","\n","    if (epoch + 1) % checkpoint_freq == 0:              # Checkpoint save\n","        checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch+1}.pt\")\n","        torch.save(model.state_dict(), checkpoint_path)\n","        print(f\"Checkpoint saved at {checkpoint_path}\")\n","\n","# SECTION 10: Plot metrics\n","\n","epochs = range(1, n_epochs+1)\n","\n","plt.figure(figsize=(15,10))\n","\n","# Loss\n","plt.subplot(2,2,1)\n","plt.plot(epochs, train_losses, label='Train Loss')\n","plt.plot(epochs, val_losses, label='Val Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training & Validation Loss')\n","plt.legend()\n","\n","# clDice and Dice\n","plt.subplot(2,2,2)\n","plt.plot(epochs, val_cldices, label='clDice', color='green')\n","plt.plot(epochs, val_dscs, label='Dice')\n","plt.xlabel('Epoch')\n","plt.ylabel('Score')\n","plt.title('Dice & clDice over Epochs')\n","plt.legend()\n","\n","# Precision and Recall\n","plt.subplot(2,2,3)\n","plt.plot(epochs, val_precs, label='Precision')\n","plt.plot(epochs, val_recalls, label='Recall')\n","plt.xlabel('Epoch')\n","plt.ylabel('Score')\n","plt.title( 'Precision & Recall')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()\n","\n"]},{"cell_type":"code","source":["from google.colab import runtime\n","runtime.unassign()"],"metadata":{"id":"qzbxUubwjcCG"},"execution_count":null,"outputs":[]}]}