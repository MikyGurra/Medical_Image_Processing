{"cells":[{"cell_type":"markdown","metadata":{"id":"sh6KdaoQIhtR"},"source":["## **Medical Image Processing - Retinal Vessel Challenge**\n","### Test code\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9KwtFF_p3fLa"},"outputs":[],"source":["# Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"rcnQrEe1I78n"},"source":["**1. Install useful libraries and U-Net definition**\n"]},{"cell_type":"code","source":["import os\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm"],"metadata":{"id":"CKYn-IWBTEtC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show versioning of deep learning libraries\n","import torch, torchvision\n","print(torch.__version__, torch.cuda.is_available())"],"metadata":{"id":"mvs7KnthTP7J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Directory that contains all the data/script\n","current_dir = \"/content/drive/MyDrive\""],"metadata":{"id":"ZJe6hbWEUuFD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class DoubleConv(nn.Module):\n","    \"\"\"Applies two consecutive conv-batchnorm-relu layers\"\"\"\n","    def __init__(self, in_channels, out_channels):\n","        super(DoubleConv, self).__init__()\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","\n","class UNet(nn.Module):\n","    def __init__(self,\n","                 in_channels=1,\n","                 out_channels=1,\n","                 init_filters=16,\n","                 depth=4,\n","                 bilinear=True):\n","        super(UNet, self).__init__()\n","        self.depth = depth\n","        self.down_layers = nn.ModuleList()\n","        self.up_layers = nn.ModuleList()\n","        self.pool = nn.MaxPool2d(2)\n","\n","        # Encoder\n","        filters = init_filters\n","        for d in range(depth):\n","            conv = DoubleConv(in_channels, filters)\n","            self.down_layers.append(conv)\n","            in_channels = filters\n","            filters *= 2\n","\n","        # Bottleneck\n","        self.bottleneck = DoubleConv(in_channels, filters)\n","\n","        # Decoder\n","        for d in range(depth):\n","            filters //= 2\n","            if bilinear:\n","                up = nn.Sequential(\n","                    nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n","                    nn.Conv2d(filters * 2, filters, kernel_size=1)\n","                )\n","            else:\n","                up = nn.ConvTranspose2d(filters * 2, filters, kernel_size=2, stride=2)\n","            self.up_layers.append(nn.ModuleDict({\n","                'up': up,\n","                'conv': DoubleConv(filters * 2, filters)\n","            }))\n","\n","        # Output layer\n","        self.out_conv = nn.Conv2d(init_filters, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        skip_connections = []\n","        for down in self.down_layers:\n","            x = down(x)\n","            skip_connections.append(x)\n","            x = self.pool(x)\n","\n","        x = self.bottleneck(x)\n","\n","        for i in range(self.depth):\n","            skip = skip_connections[-(i+1)]\n","            up = self.up_layers[i]['up'](x)\n","            if up.size() != skip.size():\n","                # Resize in case of odd size mismatch\n","                up = F.interpolate(up, size=skip.shape[2:])\n","            x = torch.cat([skip, up], dim=1)\n","            x = self.up_layers[i]['conv'](x)\n","\n","        return self.out_conv(x)"],"metadata":{"id":"lCcvIsHyTTVs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**2. Load the configuration and weights of the trained U-Net model**"],"metadata":{"id":"5ZDSRV6q2s_l"}},{"cell_type":"code","source":["import json\n","import torch\n","\n","def load_model_from_checkpoint(checkpoint_dir, epoch_to_load):\n","    # Path to the JSON file with saved parameters\n","    params_path = os.path.join(checkpoint_dir, 'training_params.json')\n","\n","    # Load parameters from JSON\n","    with open(params_path, 'r') as f:\n","        params = json.load(f)\n","\n","    print(\"Loaded parameters:\", params)\n","\n","    # Create the model using the loaded parameters\n","    model = UNet(\n","        in_channels=params['in_channels'],\n","        out_channels=params['out_channels'],\n","        init_filters=params['init_filters'],\n","        depth=params['depth']\n","    )\n","\n","    # Path to the checkpoint\n","    checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch_to_load}.pt\")\n","\n","    # Load model state\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n","    print(f\"Checkpoint for epoch {epoch_to_load} loaded from {checkpoint_path}\")\n","\n","    return model, params"],"metadata":{"id":"_P8ObGr6TpY1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint_dir = os.path.join(current_dir, 'Ale', 'ultimissima', 'checkpoints')  # checkpoint directory\n","epoch_number = 27   # epoch to load\n","\n","model, params = load_model_from_checkpoint(checkpoint_dir, epoch_number)\n","input_size = tuple(params[\"input_size\"])"],"metadata":{"id":"T1KDzvJATpO9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**3. Apply trained model to the test set**\n","\n","\n"],"metadata":{"id":"VNL89cnc26B4"}},{"cell_type":"code","source":["#from torchvision import transforms\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import cv2\n","from skimage import exposure\n","from skimage.morphology import skeletonize\n","from skimage.measure import label\n","# Define paths\n","test_images_dir = os.path.join(current_dir, 'Progetto','Dataset_vessel_stu','test','image') # input images directory (modify if necessary)\n","test_masks_dir = os.path.join(current_dir,'Progetto','Dataset_vessel_stu','test','manual_py') # manual masks directory (modify if necessary)\n","output_masks_dir = os.path.join(current_dir,'Progetto','Dataset_vessel_stu','test','predictions_final_test') # output masks directory (modify if necessary)\n","\n","# Create output folders if not present\n","os.makedirs(output_masks_dir, exist_ok=True)\n","\n","# Send model to device and set to eval\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","model.eval()\n","\n","# Preprocessing: Resize and convert to tensor\n","preprocess = A.Compose([\n","    A.Resize(height=input_size[0], width=input_size[1]),\n","    A.ToFloat(max_value=255.0),\n","    ToTensorV2()\n","])\n","# function to remove small skeleton---Post-processing function\n","def clean_by_skeleton_length(mask, min_length):\n","    labeled, num = label(mask.astype(np.uint8), return_num=True)\n","    refined = np.zeros_like(mask, dtype=np.uint8)\n","\n","    for lab in range(1, num + 1):\n","        comp_mask = (labeled == lab)\n","        skel = skeletonize(comp_mask)\n","        length = np.sum(skel)\n","\n","        if length >= min_length:\n","            refined[comp_mask] = 1\n","\n","    return refined\n","\n","# List test image files (assumes .png/.jpg/.jpeg)\n","image_files = [f for f in os.listdir(test_images_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n","\n","for img_name in image_files:\n","    img_path = os.path.join(test_images_dir, img_name)\n","\n","\n","    # Load the image and Apply all the preprocessing\n","    image = np.array(Image.open(img_path).convert('RGB'))\n","    R = image[:,:,0].astype(np.float32)\n","    G = image[:,:,1].astype(np.float32)\n","    img_RG = (0.337 * R + 0.663 * G).astype(np.uint8)\n","    #Apply Gaussian filter (kernel_size = 3; sigma =1)\n","    img = cv2.GaussianBlur(img_RG, (3,3), 1)\n","    #Apply gamma correction (gamma = 0.9)\n","    img = img/255\n","    img_gamma = exposure.adjust_gamma(img, gamma=0.9)\n","    img_gamma = (img_gamma * 255).astype(np.uint8)\n","    image_pre = preprocess(image=img_gamma)\n","    input_tensor = image_pre[\"image\"].unsqueeze(0).to(device)\n","\n","\n","    # Inference\n","    with torch.no_grad():\n","        output = model(input_tensor)\n","        pred_mask = torch.sigmoid(output).cpu().squeeze().numpy()\n","\n","\n","    # Threshold\n","    pred_mask_bin = (pred_mask > 0.5).astype(np.uint8)\n","\n","    # Post-processing: remove small components based on skeleton length\n","    pred_mask_bin = clean_by_skeleton_length(pred_mask_bin, min_length=40)\n","\n","    # Save predicted mask\n","    pred_mask_img = Image.fromarray((pred_mask_bin * 255).astype(np.uint8))\n","    pred_mask_img.save(os.path.join(output_masks_dir, img_name))"],"metadata":{"id":"jphYpDKCU9Yp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following cell calculates metrics using sklearn.metrics and the calculate_cldice function. These implementations can be replaced with alternative evaluation scripts as required."],"metadata":{"id":"e3rvC8Kt469J"}},{"cell_type":"code","source":["\n","import numpy as np\n","from PIL import Image\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","from skimage.morphology import skeletonize\n","def calculate_cldice(gt, pred): # function to compute clDice\n","\n","    # Skeleton GT and Pred\n","    skel_gt = skeletonize(gt > 0)\n","    skel_pred = skeletonize(pred > 0)\n","\n","    # True positives skeleton\n","    tprec = np.sum(skel_pred & (gt > 0)) / (np.sum(skel_pred) + 1e-8)\n","    tsens = np.sum(skel_gt & (pred > 0)) / (np.sum(skel_gt) + 1e-8)\n","\n","    cldice = 2 * tprec * tsens / (tprec + tsens + 1e-8)\n","    return cldice\n","\n","# ============================================================\n","# METRIC EVALUATION: Recall and Precision added to the standard metrics\n","# ============================================================\n","\n","Dices, Precisions, Recalls, clDices = [], [], [], []\n","\n","# sorted lists of predicted and manual masks\n","list_mask_manual = sorted([\n","    os.path.join(test_masks_dir, f)\n","    for f in os.listdir(test_masks_dir)\n","    if f.lower().endswith('.png')\n","])\n","\n","list_mask_auto = sorted([\n","    os.path.join(output_masks_dir, f)\n","    for f in os.listdir(output_masks_dir)\n","    if f.lower().endswith('.png')\n","])\n","\n","for manual_path, auto_path in tqdm(zip(list_mask_manual, list_mask_auto),\n","                                   total=len(list_mask_manual),\n","                                   desc=\"Computing test metrics\"):\n","\n","    # Ground truth\n","    manual_mask = Image.open(manual_path).convert('L')\n","    manual_mask = manual_mask.resize(input_size, Image.NEAREST)\n","    manual_mask = (np.array(manual_mask) > 0).astype(np.uint8)\n","\n","    # Predicted mask\n","    auto_mask = Image.open(auto_path).convert('L')\n","    auto_mask = auto_mask.resize(input_size, Image.NEAREST)\n","    auto_mask = (np.array(auto_mask) > 0).astype(np.uint8)\n","\n","    # Metrics\n","    flat_true = manual_mask.flatten()\n","    flat_pred = auto_mask.flatten()\n","\n","    dice = f1_score(flat_true, flat_pred, average='binary')\n","    prec = precision_score(flat_true, flat_pred, average='binary')\n","    rec  = recall_score(flat_true, flat_pred, average='binary')\n","    cld  = calculate_cldice(manual_mask, auto_mask)\n","\n","    Dices.append(dice)\n","    Precisions.append(prec)\n","    Recalls.append(rec)\n","    clDices.append(cld)\n","\n","# ============================================================\n","# Statistics\n","# ============================================================\n","\n","print(\"Dice:     mean {:.4f} std {:.4f}\".format(np.mean(Dices), np.std(Dices)))\n","print(\"Precision:mean {:.4f} std {:.4f}\".format(np.mean(Precisions), np.std(Precisions)))\n","print(\"Recall:   mean {:.4f} std {:.4f}\".format(np.mean(Recalls), np.std(Recalls)))\n","print(\"clDice:   mean {:.4f} std {:.4f}\".format(np.mean(clDices), np.std(clDices)))\n","\n","# ============================================================\n","# Boxplot\n","# ============================================================\n","\n","fig, ax = plt.subplots(1, 4, figsize=(12, 4))\n","ax[0].boxplot(Dices);      ax[0].set_title(\"Dice\")\n","ax[1].boxplot(Precisions); ax[1].set_title(\"Precision\")\n","ax[2].boxplot(Recalls);    ax[2].set_title(\"Recall\")\n","ax[3].boxplot(clDices);    ax[3].set_title(\"clDice\")\n","\n","plt.tight_layout()\n","plt.show()\n","\n"],"metadata":{"id":"8lGn6ezi46OZ"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1BdRKq8bj4fH9SCOEDqk-3cuh2Qh5h17X","timestamp":1634731372127},{"file_id":"15u0PXxVE3F5Qa5B3eKEJ2Rkf4Z1i0Q8V","timestamp":1634720935124}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}